```python
__all__ = ['ConvTimeNet_backbone']

# Cell
import copy
import torch
from torch import nn
from torch import Tensor
import torch.nn.functional as F
import numpy as np

from layers.Patch_layers import *
from layers.RevIN import RevIN

def get_activation_fn(activation):
    if callable(activation): return activation()  # 如果是可调用的，直接返回
    elif activation.lower() == "relu": return nn.ReLU()  # ReLU激活函数
    elif activation.lower() == "gelu": return nn.GELU()  # GELU激活函数
    raise ValueError(f'{activation} is not available. You can use "relu", "gelu", or a callable')  # 如果不支持，抛出异常

# Cell
class ConvTimeNet_backbone(nn.Module):
	def __init__(self, c_in:int, seq_len:int, context_window:int, target_window:int, patch_len:int, stride:int, 
				 n_layers:int=6, dw_ks=[9,11,15,21,29,39], d_model=64, d_ff:int=256, norm:str='batch', dropout:float=0., act:str="gelu", 
				 head_dropout=0, padding_patch=None, head_type='flatten', revin=True, affine=True, subtract_last=False, 
				 deformable=True, enable_res_param=True, re_param=True, re_param_kernel=3):
		
		super().__init__()
		
		# RevIn
		self.revin = revin
		if self.revin: self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)
		
		# Patching / Deformable Patching
		self.deformable = deformable
		self.patch_len = patch_len
		self.stride = stride
		self.padding_patch = padding_patch
		patch_num = int((context_window - patch_len)/stride + 1)
		if padding_patch == 'end': # can be modified to general case
			self.padding_patch_layer = nn.ReplicationPad1d((0, stride)) 
			patch_num += 1
		
		seq_len = (patch_num - 1) * self.stride + self.patch_len
		if deformable == True:
			self.deformable_sampling = DepatchSampling(c_in, seq_len, self.patch_len, self.stride)
		
		# Backbone 
		self.backbone = ConviEncoder(patch_num=patch_num, patch_len=patch_len, kernel_size=dw_ks,
								n_layers=n_layers, d_model=d_model,  d_ff=d_ff, norm=norm,
								dropout=dropout, act=act, enable_res_param=enable_res_param, 
								re_param=re_param,re_param_kernel=re_param_kernel,device='cuda:0')

		# Head
		self.head_nf = d_model * patch_num
		self.n_vars = c_in
		self.head_type = head_type

		if head_type == 'flatten': 
			self.head = Flatten_Head(self.n_vars, self.head_nf, target_window, head_dropout=head_dropout)
		else:
			raise ValueError(f'No such head@ {self.head}')
	
	def forward(self, z):  # z: [bs x nvars x seq_len]
		# norm
		if self.revin: 
			z = z.permute(0,2,1)
			z = self.revin_layer(z, 'norm')
			z = z.permute(0,2,1)
			
		# do patching
		if self.padding_patch == 'end':
			z = self.padding_patch_layer(z)

		if not self.deformable:
			z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)    
		else:
			z = self.deformable_sampling(z)
			
		z = z.permute(0,1,3,2) # z: [bs x nvars x patch_len x patch_num]
		
		# model
		z = self.backbone(z)                 # z: [bs x nvars x d_model x patch_num]
		z = self.head(z)                     # z: [bs x nvars x target_window] 
		
		# denorm
		if self.revin: 
			z = z.permute(0,2,1)
			z = self.revin_layer(z, 'denorm')
			z = z.permute(0,2,1)
		return z

class Flatten_Head(nn.Module):
	def __init__(self, n_vars, nf, target_window, head_dropout=0):
		super().__init__()
		
		self.n_vars = n_vars
		
		self.flatten = nn.Flatten(start_dim=-2)
		self.linear = nn.Linear(nf, target_window)
		self.dropout = nn.Dropout(head_dropout)
			
	def forward(self, x):     
		x = self.flatten(x)
		x = self.linear(x)
		x = self.dropout(x)
		return x
	
class ConviEncoder(nn.Module):  #i means channel-independent
	def __init__(self, patch_num, patch_len, kernel_size=[11,15,21,29,39,51], n_layers=6, d_model=128, 
				 d_ff=256, norm='batch', dropout=0., act="gelu", enable_res_param=True, 
				 re_param=True, re_param_kernel=3, device='cuda:0'):
		
		
		super().__init__()
		
		self.patch_num = patch_num
		self.patch_len = patch_len
		
		# Input embedding
		self.W_P = nn.Linear(patch_len, d_model)   

		# Residual dropout
		self.dropout = nn.Dropout(dropout)

		# Encoder
		self.encoder = ConvEncoder(kernel_size, d_model, d_ff=d_ff, norm=norm, dropout=dropout,
								   activation=act, enable_res_param=enable_res_param, n_layers=n_layers, 
								   re_param=re_param, re_param_kernel=re_param_kernel, device=device)

		
	def forward(self, x) -> Tensor:                                              # x: [bs x nvars x patch_len x patch_num]
		
		n_vars = x.shape[1]
		# Input encoding
		x = x.permute(0,1,3,2)                                                 # x: [bs x nvars x patch_num x patch_len]
		x = self.W_P(x)                                                      # x: [bs x nvars x patch_num x d_model]
		
		u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * nvars x patch_num x d_model]
		# u = self.dropout(u + self.W_pos)                                         # u: [bs * nvars x patch_num x d_model]

		# Encoder
		z = self.encoder(u.permute(0, 2, 1)).permute(0, 2, 1)                    # z: [bs * nvars x patch_num x d_model]
		z = torch.reshape(z, (-1,n_vars,z.shape[-2],z.shape[-1]))                # z: [bs x nvars x patch_num x d_model]
		z = z.permute(0,1,3,2)                                                   # z: [bs x nvars x d_model x patch_num]
		
		return z    
			
class ConvEncoder(nn.Module):
	def __init__(self, kernel_size, d_model, d_ff=None, 
						norm='batch', dropout=0., activation='gelu',
						enable_res_param=True, n_layers=3, re_param=True, re_param_kernel = 3, device='cuda:0'):
		super().__init__()

		self.layers = nn.ModuleList([ConvEncoderLayer(d_model, d_ff=d_ff, kernel_size=kernel_size[i], dropout=dropout,
													  activation=activation, enable_res_param=enable_res_param, norm=norm, 
													  re_param=re_param, small_ks=re_param_kernel, device=device) for i in range(n_layers)])

	def forward(self, src:Tensor):
		output = src
		for mod in self.layers: 
			output = mod(output)
		return output

class SublayerConnection(nn.Module):
	def __init__(self, enable_res_parameter, dropout=0.1):
		super(SublayerConnection, self).__init__()
		self.dropout = nn.Dropout(dropout)
		self.enable = enable_res_parameter
		if enable_res_parameter:
			self.a = nn.Parameter(torch.tensor(1e-8))

	def forward(self, x, out_x):
		if not self.enable:
			return x + self.dropout(out_x)  #
		else:
			return x + self.dropout(self.a * out_x)  

class ConvEncoderLayer(nn.Module):
	def __init__(self, d_model:int, d_ff:int=256, kernel_size:int=9, dropout:float=0.1, 
				 activation:str="gelu", enable_res_param=True, norm='batch', re_param=True, small_ks=3, device='cuda:0'):
		super(ConvEncoderLayer, self).__init__()

		self.norm_tp = norm
		self.re_param = re_param

		if not re_param: 
			self.DW_conv = nn.Conv1d(d_model, d_model, kernel_size, 1, 'same', groups=d_model)
		else:
			self.large_ks = kernel_size
			self.small_ks = small_ks
			self.DW_conv_large = nn.Conv1d(d_model, d_model, kernel_size, stride=1, padding='same', groups=d_model)
			self.DW_conv_small = nn.Conv1d(d_model, d_model, small_ks, stride=1, padding='same', groups=d_model)
			self.DW_infer = nn.Conv1d(d_model, d_model, kernel_size, stride=1, padding='same', groups=d_model)

		self.dw_act = get_activation_fn(activation)

		self.sublayerconnect1 = SublayerConnection(enable_res_param, dropout)
		self.dw_norm = nn.BatchNorm1d(d_model) if norm == 'batch' else nn.LayerNorm(d_model)

		# Position-wise Feed-Forward
		self.ff = nn.Sequential(nn.Conv1d(d_model, d_ff, 1, 1), 
								get_activation_fn(activation), 
								nn.Dropout(dropout), 
								nn.Conv1d(d_ff, d_model, 1, 1))

		# Add & Norm
		self.sublayerconnect2 = SublayerConnection(enable_res_param, dropout)
		self.norm_ffn = nn.BatchNorm1d(d_model) if norm == 'batch' else nn.LayerNorm(d_model)


	def _get_merged_param(self):
		left_pad = (self.large_ks - self.small_ks) // 2
		right_pad = (self.large_ks - self.small_ks) - left_pad
		module_output = copy.deepcopy(self.DW_conv_large)
		# module_output.weight += F.pad(self.DW_conv_small.weight, (left_pad, right_pad), value=0)
		module_output.weight = torch.nn.Parameter(module_output.weight +  F.pad(self.DW_conv_small.weight, (left_pad, right_pad), value=0))
		# module_output.bias += self.DW_conv_small.bias
		module_output.bias = torch.nn.Parameter(module_output.bias + self.DW_conv_small.bias)
		self.DW_infer = module_output


	def forward(self, src:torch.Tensor) -> torch.Tensor: # [B, C, L]

		## Deep-wise Conv Layer
		if not self.re_param:
			out_x = self.DW_conv(src)
		else:
			if self.training: # training phase
				large_out, small_out = self.DW_conv_large(src), self.DW_conv_small(src)
				out_x = large_out + small_out
			else: # testing phase
				self._get_merged_param()
				out_x = self.DW_infer(src)

		src2 = self.dw_act(out_x)
		# print(src2.shape); exit(0)

		src = self.sublayerconnect1(src, src2)
		src = src.permute(0, 2, 1) if self.norm_tp != 'batch' else src
		src = self.dw_norm(src)      
		src = src.permute(0, 2, 1) if self.norm_tp != 'batch' else src

		## Position-wise Conv Feed-Forward
		src2 = self.ff(src)
		src2 = self.sublayerconnect2(src, src2) # Add: residual connection with residual dropout

		# Norm: batchnorm or layernorm
		src2 = src2.permute(0, 2, 1) if self.norm_tp != 'batch' else src2
		src2 = self.norm_ffn(src2)      
		src2 = src2.permute(0, 2, 1) if self.norm_tp != 'batch' else src2
			
		return src




import torch, math
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class BoxCoder(nn.Module):
	def __init__(self, patch_count, patch_stride, patch_size, seq_len, channels, device='cuda:0'):
		super().__init__()
		self.device = device

		self.seq_len = seq_len
		self.channels = channels
		self.patch_size = patch_size
		self.patch_count = patch_count
		self.patch_stride = patch_stride
		
		self._generate_anchor(device=device)
		
	# compute the center points. idx: [0 ~ seq_len - 1]
	def _generate_anchor(self, device="cuda:0"):
		anchors = []
		self.S_bias = (self.patch_size - 1) / 2
		
		for i in range(self.patch_count):
			x = i * self.patch_stride + 0.5 * (self.patch_size - 1)
			anchors.append(x)

		# anchors = torch.as_tensor(anchors, device=device)
		anchors = torch.as_tensor(anchors, device='cpu')
		self.register_buffer("anchor", anchors)

	def forward(self, boxes):
		self.bound = self.decode(boxes) # (bs, patch_count, channel, 2)
		points = self.meshgrid(self.bound)

		return points, self.bound

	def decode(self, rel_codes):  # Input: (B, patch_count, channel, 2)
		boxes = self.anchor

		dx = rel_codes[:, :, :, 0]
		ds = torch.relu(rel_codes[:, :, :, 1] + self.S_bias)

		pred_boxes = torch.zeros_like(rel_codes)
		ref_x = boxes.view(1, boxes.shape[0], 1)

		# dx, ds: (bs, patch_count, channel, 1)
		
		pred_boxes[:, :, :, 0] = (dx + ref_x - ds) 
		pred_boxes[:, :, :, 1] = (dx + ref_x + ds) 
		pred_boxes /= (self.seq_len - 1)

		pred_boxes = pred_boxes.clamp_(min=0., max=1.)

		# pred_boxes: each of the patch's left-bound & right-bound. norm to [0, 1]
		return pred_boxes	
   
	def meshgrid(self, boxes): # Input: pred_boxes. To get the sampling location
		B, patch_count, C = boxes.shape[0], boxes.shape[1], boxes.shape[2]
		# channel_boxes = torch.zeros((boxes.shape[0], boxes.shape[1], 2)).to(self.device)
		channel_boxes = torch.zeros((boxes.shape[0], boxes.shape[1], 2))
		channel_boxes[:, :, 1] = 1.0
		xs = boxes.view(B*patch_count, C, 2)
		xs = torch.nn.functional.interpolate(xs, size=self.patch_size, mode='linear', align_corners=True)
		ys = torch.nn.functional.interpolate(channel_boxes, size=self.channels, mode='linear', align_corners=True)

		# xs: [bs, patch_count, channel, patch_size]   ys: [bs, patch_count, channels(also feats)]
  
		xs = xs.view(B, patch_count, C, self.patch_size, 1)
		ys = ys.unsqueeze(3).expand(B, patch_count, C, self.patch_size).unsqueeze(-1)
  
		grid = torch.stack([xs, ys], dim = -1)
		return grid # [bs, patch_count, channel, patch_size, 2]

def zero_init(m):
	if type(m) == nn.Linear or type(m) == nn.Conv1d:
		m.weight.data.fill_(0)
		m.bias.data.fill_(0)

class OffsetPredictor(nn.Module):
	def __init__(self, in_feats, patch_size, stride, use_zero_init=True):
		"""
		Note: decoupling on channel-dim !
  		"""
		super().__init__()
		self.stride = stride
		self.channel = in_feats
		self.patch_size = patch_size

		self.offset_predictor = nn.Sequential(
			nn.Conv1d(1, 64, patch_size, stride=stride, padding=0), 
			nn.GELU(),
			nn.Conv1d(64, 2, 1, 1, padding=0) 
		)

		if use_zero_init:
			self.offset_predictor.apply(zero_init)
		
	def forward(self, X): # Input: (bs, channel, seq_len)
		
		patch_X = X.unsqueeze(1).permute(0, 1, 3, 2)
		patch_X = F.unfold(patch_X, kernel_size=(self.patch_size, self.channel), stride=self.stride).permute(0, 2, 1) # (B, patch_count, patch_size*channel)

		# decoupling
		B, patch_count = patch_X.shape[0], patch_X.shape[1] 
		patch_X = patch_X.contiguous().view(B, patch_count, self.patch_size, self.channel)
		patch_X = patch_X.permute(0, 1, 3, 2)

		# patch_X: (B, patch_count, channel, patchsize)
		patch_X = patch_X.contiguous().view(B*patch_count*self.channel, 1, self.patch_size)

		# calculate the bias throughout 2 Conv1d
		pred_offset = self.offset_predictor(patch_X)
		pred_offset = pred_offset.view(B, patch_count, self.channel, 2).contiguous()

		# For each of the patch block and it's channel, there exists a bias（dx, ds）
		# pred_offset: (B, patch_count, channel, 2)
		return pred_offset 

# Input: (B, C, L)  Output: (B, C, patch_num * patch_len)
class DepatchSampling(nn.Module):
	def __init__(self, in_feats, seq_len, patch_size, stride):	 
		super(DepatchSampling, self).__init__()
		self.in_feats = in_feats
		self.seq_len = seq_len
		self.patch_size = patch_size

		self.patch_count = (seq_len - patch_size) // stride + 1
  
		self.dropout = nn.Dropout(0.1)
  
		# offset predictor
		self.offset_predictor = OffsetPredictor(in_feats, patch_size, stride)

		self.box_coder = BoxCoder(self.patch_count, stride, patch_size, self.seq_len, in_feats)
  
	def get_sampling_location(self, X): # Input: (bs, channel, window)
		"""
		Input shape: (bs, channel, window) ;
		Sampling location  shape: [bs, patch_count, C, self.patch_size, 2]. range = [0, 1] ; 
		"""
		# get offset
		pred_offset = self.offset_predictor(X)

		sampling_locations, bound = self.box_coder(pred_offset)
		return sampling_locations, bound
	
	def forward(self, X, return_bound=False): # Input: (bs, channel, window)
		# Consider the X as a img. shape: (B, C, H, W) <--> (bs, 1, channel, padded_window)
		img = X.unsqueeze(1)
		B = img.shape[0]

		sampling_locations, bound = self.get_sampling_location(X) # sampling_locations: [bs, patch_count, channel, patch_size, 2]
		
		# 计算目标形状
		target_shape = (B, self.patch_count * self.in_feats, self.patch_size, 2)
		if np.prod(target_shape) != sampling_locations.numel():
			raise ValueError(f"目标形状 {target_shape} 与输入大小 {sampling_locations.numel()} 不匹配")

		sampling_locations = sampling_locations.view(*target_shape)

		# print('sampling_locations: ', sampling_locations.shape)

		sampling_locations = (sampling_locations - 0.5) * 2 # location map: [-1, 1]
		output = F.grid_sample(img, sampling_locations, align_corners=True) 
		output = output.view(B, self.patch_count, self.in_feats, self.patch_size)
		output = output.permute(0, 2, 1, 3).contiguous()
		return output # (B, C, patch_count, patch_size)





import torch
import torch.nn as nn

class RevIN(nn.Module):
    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):
        """
        :param num_features: the number of features or channels
        :param eps: a value added for numerical stability
        :param affine: if True, RevIN has learnable affine parameters
        """
        super(RevIN, self).__init__()
        self.num_features = num_features
        self.eps = eps
        self.affine = affine
        self.subtract_last = subtract_last
        if self.affine:
            self._init_params()

    def forward(self, x, mode:str):
        if mode == 'norm':
            self._get_statistics(x)
            x = self._normalize(x)
        elif mode == 'denorm':
            x = self._denormalize(x)
        else: raise NotImplementedError
        return x

    def _init_params(self):
        # initialize RevIN params: (C,)
        self.affine_weight = nn.Parameter(torch.ones(self.num_features))
        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))

    def _get_statistics(self, x):
        dim2reduce = tuple(range(1, x.ndim-1))
        if self.subtract_last:
            self.last = x[:,-1,:].unsqueeze(1)
        else:
            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()

    def _normalize(self, x):
        if self.subtract_last:
            x = x - self.last
        else:
            x = x - self.mean
        x = x / self.stdev
        if self.affine:
            x = x * self.affine_weight
            x = x + self.affine_bias
        return x

    def _denormalize(self, x):
        if self.affine:
            x = x - self.affine_bias
            x = x / (self.affine_weight + self.eps*self.eps)
        x = x * self.stdev
        if self.subtract_last:
            x = x + self.last
        else:
            x = x + self.mean
        return x

__all__ = ['ConvTimeNet']

# 导入必要的库
import torch
from torch import nn
from layers.ConvTimeNet_backbone import ConvTimeNet_backbone

# ConvTimeNet: depatch + batch norm + gelu + Conv + 2-layer-ffn(PointWise Conv + PointWise Conv)
class Model(nn.Module):
    def __init__(self, configs, norm:str='batch', act:str="gelu", head_type='flatten'):
        super().__init__()
        
        # 加载参数
        c_in = configs.enc_in  # 输入通道数
        context_window = configs.seq_len  # 上下文窗口大小
        target_window = configs.pred_len  # 目标窗口大小
        
        n_layers = configs.e_layers  # 网络层数
        d_model = configs.d_model  # 模型维度
        d_ff = configs.d_ff  # 前馈网络维度
        dropout = configs.dropout  # dropout 概率
        head_dropout = configs.head_dropout  # 头部 dropout 概率
    
        patch_len = configs.patch_ks  # patch 大小
        stride = configs.patch_sd  # 步幅
        padding_patch = configs.padding_patch  # patch 填充方式
        
        revin = configs.revin  # 是否使用逆归一化
        affine = configs.affine  # 是否使用仿射变换
        subtract_last = configs.subtract_last  # 是否减去最后一个值
        
        seq_len = configs.seq_len  # 序列长度
        dw_ks = configs.dw_ks  # 深度可分离卷积核大小

        re_param = configs.re_param  # 重新参数化
        re_param_kernel = configs.re_param_kernel  # 重新参数化卷积核大小
        enable_res_param = configs.enable_res_param  # 是否启用残差参数化
    
        # 初始化模型
        self.model = ConvTimeNet_backbone(
            c_in=c_in, 
            seq_len=seq_len, 
            context_window=context_window,
            target_window=target_window, 
            patch_len=patch_len, 
            stride=stride, 
            n_layers=n_layers, 
            d_model=d_model, 
            d_ff=d_ff, 
            dw_ks=dw_ks, 
            norm=norm, 
            dropout=dropout, 
            act=act,
            head_dropout=head_dropout, 
            padding_patch=padding_patch, 
            head_type=head_type, 
            revin=revin, 
            affine=affine, 
            deformable=True, 
            subtract_last=subtract_last, 
            enable_res_param=enable_res_param, 
            re_param=re_param, 
            re_param_kernel=re_param_kernel
        )
        
    def forward(self, x):
        x = x.permute(0, 2, 1)  # 调整张量维度顺序
        x = self.model(x)  # 通过模型进行前向传播
        x = x.permute(0, 2, 1)  # 恢复张量维度顺序

        return x  # 返回输出



import argparse  # 导入argparse模块，用于命令行参数解析
import os  # 导入os模块，用于操作系统相关功能
import torch  # 导入PyTorch库
from exp.exp_main import Exp_Main  # 从exp_main模块中导入Exp_Main类
import random  # 导入random模块，用于生成随机数
import numpy as np  # 导入NumPy库，用于科学计算
import sys  # 导入sys模块，用于系统相关操作
from tqdm import tqdm  # 导入tqdm库
os.chdir(sys.path[0])  # 将当前工作目录更改为脚本所在目录，以确保相对路径正确

# 创建ArgumentParser对象，用于处理命令行参数
parser = argparse.ArgumentParser(description='ConvTimeNet for Time Series Forecasting')

# 设置随机种子参数，确保实验的可重复性
parser.add_argument('--random_seed', type=int, default=2021, help='random seed')

# 基本配置参数
parser.add_argument('--is_training', type=int, required=False, default=1, help='status')  # 是否处于训练模式
parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')  # 模型标识符
parser.add_argument('--model', type=str, required=False, default='ConvTimeNet',
                    help='model name, options: [Autoformer, Informer, Transformer]')  # 模型名称

# 数据加载器参数
parser.add_argument('--data', type=str, required=False, default='ETTh1', help='dataset type')  # 数据集类型
parser.add_argument('--root_path', type=str, default='../dataset/', help='root path of the data file')  # 数据文件根路径
# parser.add_argument('--root_path', type=str, default='E:/浏览器下载地址/宁海疾控食源性疾病/TimeNet/dataset/', help='root path of the data file')  # 数据文件根路径
# parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')  # 数据文件名
parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')  # 数据文件名
parser.add_argument('--features', type=str, default='M',
                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')  # 预测任务类型
parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')  # 目标特征
parser.add_argument('--freq', type=str, default='h',
                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')  # 时间特征编码的频率
parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')  # 模型检查点位置

# 预测任务参数
parser.add_argument('--seq_len', type=int, default=2, help='input sequence length')  # 输入序列长度
parser.add_argument('--label_len', type=int, default=1, help='start token length')  # 标签长度
parser.add_argument('--pred_len', type=int, default=2, help='prediction sequence length')  # 预测序列长度

# ConvTimeNet相关参数
parser.add_argument('--head_dropout', type=float, default=0.0, help='head dropout')  # 头部dropout
parser.add_argument('--padding_patch', default='end', help='None: None; end: padding on the end')  # 补丁填充方式
parser.add_argument('--revin', type=int, default=1, help='RevIN; True 1 False 0')  # 是否使用RevIN
parser.add_argument('--affine', type=int, default=0, help='RevIN-affine; True 1 False 0')  # RevIN的仿射变换
parser.add_argument('--subtract_last', type=int, default=0, help='0: subtract mean; 1: subtract last')  # 减去最后一个值或均值

parser.add_argument('--dw_ks', type=str, default='11,15,21,29,39,51', help="kernel size of the deep-wise. default:9")  # 深度卷积核大小
parser.add_argument('--re_param', type=int, default=1, help='Reparam the DeepWise Conv when train')  # 训练时重新参数化深度卷积
parser.add_argument('--enable_res_param', type=int, default=1, help='Learnable residual')  # 可学习的残差
parser.add_argument('--re_param_kernel', type=int, default=3)  # 重新参数化的卷积核大小

# 补丁相关参数
parser.add_argument('--patch_ks', type=int, default=3, help="kernel size of the patch window. default:32")  # 补丁窗口的卷积核大小
parser.add_argument('--patch_sd', type=float, default=0.5, \
                    help="stride of the patch window. default: 0.5. if < 1, then sd = patch_sd * patch_ks")  # 补丁窗口的步幅

# 其他参数
parser.add_argument('--enc_in', type=int, default=7, help='encoder input size')  # 编码器输入大小
parser.add_argument('--c_out', type=int, default=7, help='output size')  # 输出大小
parser.add_argument('--d_model', type=int, default=64, help='dimension of model')  # 模型维度
parser.add_argument('--e_layers', type=int, default=6, help='num of encoder layers')  # 编码器层数
parser.add_argument('--d_ff', type=int, default=256, help='dimension of fcn')  # 全连接层的维度

parser.add_argument('--dropout', type=float, default=0.05, help='dropout')  # dropout率
parser.add_argument('--embed', type=str, default='timeF',
                    help='time features encoding, options:[timeF, fixed, learned]')  # 时间特征编码方式
parser.add_argument('--activation', type=str, default='gelu', help='activation')  # 激活函数
parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')  # 是否预测未见的未来数据

# 优化参数
parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')  # 数据加载器的工作线程数
parser.add_argument('--itr', type=int, default=2, help='experiments times')  # 实验次数
parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')  # 训练轮数
parser.add_argument('--batch_size', type=int, default=1, help='batch size of train input data')  # 训练输入数据的批次大小
parser.add_argument('--patience', type=int, default=3, help='early stopping patience')  # 提前停止的耐心
parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')  # 优化器学习率
parser.add_argument('--des', type=str, default='test', help='exp description')  # 实验描述
parser.add_argument('--loss', type=str, default='mse', help='loss function')  # 损失函数
parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)  # 是否使用自动混合精度训练
# 在数据加载器参数部分添加 scale 参数
parser.add_argument('--scale', type=bool, default=True, help='whether to scale the data')  # 数据是否标准化

# GPU相关参数
parser.add_argument('--CTX', type=str, default='0', required=False, help='visuable device ids')  # 可见设备ID
# parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')  # 是否使用GPU
parser.add_argument('--use_gpu', type=bool, default=False, help='use gpu')  # 是否使用GPU
parser.add_argument('--gpu', type=int, default=0, help='gpu')  # GPU编号
parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)  # 是否使用多GPU
# parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')  # 多GPU的设备ID
parser.add_argument('--devices', type=str, default='cpu', help='device ids of multile gpus')  # 多GPU的设备ID
parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')  # 是否测试FLOP

if __name__ == '__main__':
    # 解析命令行参数
    args = parser.parse_args()

    # 设置CUDA可见设备
    os.environ["CUDA_VISIBLE_DEVICES"] = args.CTX

    # 设置随机种子，确保实验的可重复性
    fix_seed = args.random_seed
    random.seed(fix_seed)
    torch.manual_seed(fix_seed)
    np.random.seed(fix_seed)

    # 检查是否使用GPU
    args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False

    # 如果使用多GPU
    if args.use_gpu and args.use_multi_gpu:
        args.dvices = args.devices.replace(' ', '')  # 去除设备ID中的空格
        device_ids = args.devices.split(',')  # 分割设备ID
        args.device_ids = [int(id_) for id_ in device_ids]  # 转换为整数列表
        args.gpu = args.device_ids[0]  # 设置主GPU

    # 解析深度卷积核大小
    args.dw_ks = [int(ks) for ks in args.dw_ks.split(',')]
    # 计算补丁步幅
    args.patch_sd = max(1, int(args.patch_ks * args.patch_sd)) if args.patch_sd <= 1 else int(args.patch_sd)
    # 确保编码器层数与深度卷积核列表长度匹配
    assert args.e_layers == len(args.dw_ks), "e_layers should match the dw kernel list!"

    # 打印实验参数
    print('Args in experiment:')
    print(args)

    # 实验主类
    Exp = Exp_Main
    mses = []  # 存储均方误差
    maes = []  # 存储平均绝对误差

    # 如果是训练模式
    if args.is_training:
        for ii in range(args.itr):
            # 设置实验记录
            setting = '{}_dm{}_df{}_el{}_dk{}_pk{}_ps{}_erp{}_rp{}_{}_{}'.format(
                args.model_id,
                args.d_model,
                args.d_ff,
                args.e_layers,
                args.dw_ks, 
                args.patch_ks,
                args.patch_sd,
                args.enable_res_param,
                args.re_param,
                args.des,ii)

            exp = Exp(args)  # 设置实验
            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))
            
            # 使用tqdm显示训练进度
            for epoch in tqdm(range(args.train_epochs), desc="Training Progress"):
                exp.train(setting)  # 开始训练

            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))
            
            # 使用tqdm显示测试进度
            for _ in tqdm(range(1), desc="Testing Progress"):
                mse, mae = exp.test(setting)  # 测试模型
            
            mses.append(mse)  # 记录均方误差
            maes.append(mae)  # 记录平均绝对误差

            torch.cuda.empty_cache()  # 清空CUDA缓存

        # 打印平均误差
        print('average mse:{0:.3f}±{1:.3f}, mae:{2:.3f}±{3:.3f}'.format(np.mean(
        mses), np.std(mses), np.mean(maes), np.std(maes))) 
    else:
        ii = 0
        setting = '{}_dm{}_df{}_el{}_dk{}_pk{}_ps{}_erp{}_rp{}_{}_{}'.format(
            args.model_id,
            args.d_model,
            args.d_ff,
            args.e_layers,
            args.dw_ks, 
            args.patch_ks,
            args.patch_sd,
            args.enable_res_param,
            args.re_param,
            args.des,ii)

        exp = Exp(args)  # 设置实验
        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))
        
        # 使用tqdm显示测试进度
        for _ in tqdm(range(1), desc="Testing Progress"):
            exp.test(setting, test=1)  # 测试模型
        
        torch.cuda.empty_cache()  # 清空CUDA缓存

```

上述是我的模型设计，请你重新设计模型，要求模型有效且轻量化，要求模型计算效率高，可以在CPU上运行并且运行速度快，请简化上述模型结构